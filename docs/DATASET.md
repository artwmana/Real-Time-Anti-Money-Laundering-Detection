# AMLNet Transaction Dataset (v1)

Synthetic Anti–Money-Laundering (AML) transaction dataset generated by the **AMLNet multi-agent simulation framework** (Zenodo: 10.5281/zenodo.16736515). This dataset is intended for AML research, fraud detection, risk modeling, and simulation of financial behavior

---

## 1. Overview

AMLNet is a simulator of a financial ecosystem in which agents (customers, businesses, and money launderers) interact, producing realistic transaction patterns:

* **1,090,173** transactions
* **~4,927 customers**
* **~0.16%** marked as *money laundering*
* Labels are produced by the simulation engine and are guaranteed consistent
* Fully synthetic—safe for analysis and publication

---

## 2. File Structure

```
data/
├── raw/
│   └── AMLNet_August_2025.csv            # raw dataset exported from Zenodo
└── processed/
    ├── train.parquet
    ├── val.parquet
    └── test.parquet
docs/
├──dataset.yml                             # schema + preprocessing config
configs/
├──DATASET.md                              # documentation (this file)
```

---

## 3. Schema (Columns)

### Required Columns

| Column            | Type    | Description |
| ----------------- | ------- | ----------- |
| step              | int64   | Simulation step (time index) |
| type              | object  | Payment method (TRANSFER, OSKO, BPAY, EFTPOS, DEBIT, NPP) |
| amount            | float64 | Transaction amount in AUD |
| category          | object  | Transaction category |
| nameOrig          | object  | Originating customer ID |
| nameDest          | object  | Destination customer/merchant ID |
| oldbalanceOrg     | float64 | Balance before transaction |
| newbalanceOrig    | float64 | Balance after transaction |
| isFraud           | int64   | Binary fraud indicator (0/1) |
| isMoneyLaundering | int64   | Binary AML label (0/1) |
| metadata          | object  | Nested JSON with timestamp/location/device/etc. |

### Derived columns (pipeline)
Filled in automatically `FeaturePipeline`:
| Column        | Type    | Origin |
| ------------- | ------- | ------ |
| timestamp     | datetime| from metadata |
| hour          | int64   | from timestamp |
| day_of_week   | int64   | from timestamp |
| is_weekend    | int8    | feature engineering |
| hour_sin/cos  | float64 | time encoding |
| dow_sin/cos   | float64 | time encoding |
| balance_delta | float64 | oldbalanceOrg - newbalanceOrig |
| amt_to_balance| float64 | amount / oldbalanceOrg (safe divide) |
| timestamp_ts  | int64   | unix seconds (used for time folds) |

### Metadata
`metadata` — JSON that is parsed and flattened by the `flatten_metadata` function (see `aml.features.json_extractor`). Key fields: `timestamp` (datetime string), `location` (city/state/country/postcode), `device_info` (type/os/ip_address), `payment_method`, `merchant_info`, `risk_indicators` (numeric/boolean).

### Target

| Column              | Type   | Description                               |
| ------------------- | ------ | ----------------------------------------- |
| isFraud | bool | 1 — fraud, 0 — normal transaction |
| isMoneyLaundering | bool | 1 — suspicious/ML, 0 — normal transaction |
---

## 4. What `FeaturePipeline` does

1) Input validation and metadata unwrapping (`flatten_metadata`), optionally dropping the original field
2) Providing time columns: parses `timestamp`, adds `hour`, `day_of_week`
3) Downcasting numeric types (`optimize_dataframe`)
4) Feature engineering: `is_weekend`, sine/cosine for hour and day of week, `balance_delta`, `amt_to_balance`
5) Finalization: convert `timestamp` to `timestamp_ts`, replaces `inf` with `nan`, pads numeric `NaN` with zeros.
6) Encoding (if enabled):
    - Numeric -> `SimpleImputer(median)` -> optionally `RobustScaler`
    - Low cardinality (≤8 unique) -> `SimpleImputer(most_frequent)` -> `OneHotEncoder(handle_unknown="ignore")`
    - Original -> `SimpleImputer(most_frequent)` -> `OrdinalEncoder(min_frequency=50, unknown_value=-1)`
    - High cardinality -> `TimeKFoldTargetEncoder` (time-aware target encoding with rare classes and smoothing; provides an encoding and, optionally, a counter)
    - All steps are collected in `ColumnTransformer`

---

## 5. Feature Groups (after `FeaturePipeline`)

### Numeric
- step (if present), amount, oldbalanceOrg, newbalanceOrig
- balance_delta, amt_to_balance
- hour, day_of_week, is_weekend
- hour_sin, hour_cos, dow_sin, dow_cos
- timestamp_ts (used only as a time context for target encoding, then encoded numerically)

### Categorical
- Low cardinality (example): type, laundering_typology, payment_method, loc_city/state/country, device_type/os (if present)
- High cardinality: category, nameOrig, nameDest, device_ip_address, merchant_id, and other identifiers

---

## 6. Data Cleaning Rules

| Rule | Behavior |
| ------------------------ | -------- |
| Metadata anomalies | `normalize_python_json_string` -> `ast.literal_eval` |
| Object NaNs | fill `Unknown` in loc/device/merchant/risk blocks |
| Invalid numerics | `pd.to_numeric(errors='coerce')` + median imputation |
| Ratio infinities | Replace with 0 after `replace([inf, -inf], nan)` and `fillna(0)` |

---

## 7. Encoding

- **Numeric** -> `SimpleImputer(median)` -> optionally `RobustScaler`
- **Low cardinality** (≤8 unique) -> `SimpleImputer(most_frequent)` -> `OneHotEncoder(handle_unknown="ignore")`
- **Original** -> `SimpleImputer(most_frequent)` -> `OrdinalEncoder(min_frequency=50, unknown_value=-1)`
- **High cardinality** -> `TimeKFoldTargetEncoder` (time-aware target encoding with rare classes and smoothing; provides an encoding and, optionally, a counter)
- Everything is collected in `ColumnTransformer`; when `enable_columns=True` the names of the new features are returned
---

## 8. Typical ML Workflow

1. Loading and basic cleaning (parsing metadata, time columns).
2. Running through FeaturePipeline.fit_transform on the train (custom split if needed).
3. Training a model (CatBoost/LightGBM/XGBoost) on the encoded features.
4. Pipeline.transform for validation/production, drift monitoring, and the proportion of rare categories.

----

## 9. Citation

```
AMLNet Synthetic Dataset (2024). Zenodo. https://doi.org/10.5281/zenodo.16736515
```

---

## 10. License

The dataset is synthetic and freely available for research. See the Zenodo page for license details.
